{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify - Data Lake using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22845ae81dd4aa19067adc67b2592e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1589142924251_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-28-224.us-west-2.compute.internal:20888/proxy/application_1589142924251_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-28-127.us-west-2.compute.internal:8042/node/containerlogs/container_1589142924251_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import configparser\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bacfbf93eaa745cc925959c5a4e5ca25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached boto3-1.13.6-py2.py3-none-any.whl (128 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting botocore<1.17.0,>=1.16.6\n",
      "  Using cached botocore-1.16.6-py2.py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/site-packages (from boto3) (0.9.4)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Collecting urllib3<1.26,>=1.20; python_version != \"3.4\"\n",
      "  Using cached urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.6->boto3) (1.13.0)\n",
      "Installing collected packages: docutils, urllib3, python-dateutil, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.13.6 botocore-1.16.6 docutils-0.15.2 python-dateutil-2.8.1 s3transfer-0.3.3 urllib3-1.25.9"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package('boto3')\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd70cb9354841468a46108abd502ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from  pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import (udf, col, year, month, dayofmonth, hour,\n",
    "    weekofyear, date_format, dayofweek, max, monotonically_increasing_id)\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, TimestampType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Stage and Application Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe28af642980499a979b1d0d8caa3ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "APP = 'sparkify'\n",
    "STAGE = 'dev'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('aa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up AWS Env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_file(filename:str):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(filename)\n",
    "    print(config)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = './dl.cfg'\n",
    "config = load_config_file(config_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bucket to Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name:str, region:str=os.getenv('AWS_DEFAULT_REGION'), acl=\"private\"):\n",
    "    \"\"\"\n",
    "    Creates a bucket name in the specified region\n",
    "    \n",
    "    Boto3 will make use of the set environment variables or assumed role.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name {str} -- Bucket to create\n",
    "        region {str} -- String region to create bucket in, defaults to AWS_DEFAULT_REGION in env variable\n",
    "        acl {str} -- Canned access control list to apply to the bucket. 'public-read'\n",
    "            makes sure everything posted is publicly readable\n",
    "    Returns:\n",
    "        {str} -- bucket_name\n",
    "    \"\"\"\n",
    "    \n",
    "    assert region, \"No region was defined\"\n",
    "    \n",
    "    # Create bucket\n",
    "    try:\n",
    "        s3_client = boto3.client('s3', region_name=region)\n",
    "        location = {'LocationConstraint': region}\n",
    "        s3_client.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration=location\n",
    "            )\n",
    "            \n",
    "        logger.info(f\"Bucket `{bucket_name}` Created\")\n",
    "    except ClientError as e:\n",
    "        logging.exception(\"Could not create bucket\")\n",
    "        raise e\n",
    "        \n",
    "    return bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5dc711f7a840fe8b7d6a684ee76f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_bucket_name = f'{APP}-{STAGE}'\n",
    "# create_bucket(output_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'\n",
    "def create_spark_session():\n",
    "    \"\"\"Create a Spark session\"\"\"\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be5d6319a2c42298ebe736f51465f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "song_data_path =  \"s3a://udacity-dend/song_data\" #/A/A/A\n",
    "log_data_path = \"s3a://udacity-dend/log_data\" #2018/11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Spark DF Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c719e524e9d409d940e934bde647a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SparkDF(object):\n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "    \n",
    "    def _load_json_data(self):\n",
    "        location = f\"{self.location}/*.json\"\n",
    "        return self.spark.read.json(location, schema=self.schema)\n",
    "    \n",
    "    def _write_to_parquet(self, s3_output_path:str, mode:str='overwrite', partitions:list=[]):\n",
    "        self.df.write.parquet(\n",
    "            s3_output_path,\n",
    "            mode=mode,\n",
    "            partitionBy=partitions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fe55f3f58047e7900c84ad3785cf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class RawDF(SparkDF):\n",
    "    def __init__(self,spark_session, location:str, data_schema:StructType):\n",
    "        super().__init__(spark_session)\n",
    "        self.location = location\n",
    "        self.schema = data_schema\n",
    "        self.df = self._load_json_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba22179d50d417e8b9e60444fbbb27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DerivativeDF(SparkDF):\n",
    "    def __init__(self, df:DataFrame):\n",
    "        self.df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Song Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Song Spark Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5c80ce7c5e4cdd9a1576ab1ff83a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_song_schema():\n",
    "    return StructType([\n",
    "            StructField(\"artist_id\", StringType(), False),\n",
    "            StructField(\"artist_latitude\", StringType(), True),\n",
    "            StructField(\"artist_longitude\", StringType(), True),\n",
    "            StructField(\"artist_location\", StringType(), True),\n",
    "            StructField(\"artist_name\", StringType(), False),\n",
    "            StructField(\"song_id\", StringType(), False),\n",
    "            StructField(\"title\", StringType(), False),\n",
    "            StructField(\"duration\", DoubleType(), False),\n",
    "            StructField(\"year\", IntegerType(), False)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3af0d0e43e4480802ff207c34225ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "song_raw = RawDF(spark,f\"{song_data_path}/*/*/*\", get_song_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Songs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9503dd203644938c7a81aa1423d675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songs = DerivativeDF(song_raw.df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18b7473c8ec4b8e981b4599cad61319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songs._write_to_parquet(\n",
    "    s3_output_path=f\"s3://{output_bucket_name}/songs\",\n",
    "    partitions=[\"year\", \"artist_id\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Artists Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa77aa1de2b49739d3c53dc260b5ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "artists = DerivativeDF(song_raw.df\n",
    "        .select(\n",
    "            \"artist_id\",\n",
    "            col(\"artist_name\").alias(\"name\"),\n",
    "            col(\"artist_location\").alias(\"location\"),\n",
    "            col(\"artist_latitude\").alias(\"latitude\"),\n",
    "            col(\"artist_longitude\").alias(\"longitude\"))\n",
    "        .distinct()\n",
    "    )\n",
    "artists._write_to_parquet(\n",
    "    s3_output_path=f\"s3://{output_bucket_name}/artists\",\n",
    "    partitions=[\"artist_id\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Log Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Spark Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428a26584d4043aaa3de22c308c5a9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_log_schema():\n",
    "    return StructType([\n",
    "        StructField(\"artist\", StringType(), True),\n",
    "        StructField(\"auth\", StringType(), False),\n",
    "        StructField(\"firstName\", StringType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"itemInSession\", IntegerType(), False),\n",
    "        StructField(\"lastName\", StringType(), True),\n",
    "        StructField(\"length\", DoubleType(), True),\n",
    "        StructField(\"level\", StringType(), False),\n",
    "        StructField(\"location\", StringType(), True),\n",
    "        StructField(\"method\", StringType(), False),\n",
    "        StructField(\"page\", StringType(), False),\n",
    "        StructField(\"registration\", DoubleType(), True),\n",
    "        StructField(\"sessionId\", IntegerType(), False),\n",
    "        StructField(\"song\", StringType(), True),\n",
    "        StructField(\"status\", IntegerType(), False),\n",
    "        StructField(\"ts\", DoubleType(), False),\n",
    "        StructField(\"userAgent\", StringType(), True),\n",
    "        StructField(\"userId\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create UDF to add timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1b2e5ec8994689956be03139a9af79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_datetime = udf(\n",
    "        lambda x: datetime.fromtimestamp(x / 1000).replace(microsecond=0),\n",
    "        TimestampType()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1531507d6ae848caabf1e813bb97dc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_raw = RawDF(spark, f\"{log_data_path}/*/*\", get_log_schema())\n",
    "log_raw.df = log_raw.df.filter(col(\"page\") == \"NextSong\")\n",
    "log_raw.df = log_raw.df.withColumn(\"start_time\", get_datetime(\"ts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47e6edace4942e3920533069af2fcf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users = DerivativeDF(log_raw.df\n",
    "        .withColumn(\"max_ts_user\", max(\"ts\").over(Window.partitionBy(\"userID\")))\n",
    "        .filter(\n",
    "            (col(\"ts\") == col(\"max_ts_user\")) &\n",
    "            (col(\"userID\") != \"\") &\n",
    "            (col(\"userID\").isNotNull())\n",
    "        )\n",
    "        .select(\n",
    "            col(\"userID\").alias(\"user_id\"),\n",
    "            col(\"firstName\").alias(\"first_name\"),\n",
    "            col(\"lastName\").alias(\"last_name\"),\n",
    "            \"gender\",\n",
    "            \"level\"\n",
    "        )\n",
    "    )\n",
    "users._write_to_parquet(\n",
    "    s3_output_path=f\"s3://{output_bucket_name}/users\",\n",
    "    partitions=[\"level\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bac3849c2d45ff9d3837c2a318021d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time = DerivativeDF(log_raw.df\n",
    "        .withColumn(\"hour\", hour(\"start_time\"))\n",
    "        .withColumn(\"day\", dayofmonth(\"start_time\"))\n",
    "        .withColumn(\"week\", weekofyear(\"start_time\"))\n",
    "        .withColumn(\"month\", month(\"start_time\"))\n",
    "        .withColumn(\"year\", year(\"start_time\"))\n",
    "        .withColumn(\"weekday\", dayofweek(\"start_time\"))\n",
    "        .select(\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")\n",
    "        .distinct()\n",
    "         )\n",
    "time._write_to_parquet(\n",
    "    s3_output_path=f\"s3://{output_bucket_name}/time\",\n",
    "    partitions=[\"year\", \"month\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create SongsPlay Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88031d732d14180b8f311c7d652f547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songs_temp = (\n",
    "        songs.df\n",
    "        .join(artists.df, \"artist_id\", \"full\")\n",
    "        .select(\"song_id\", \"title\", \"artist_id\", \"name\", \"duration\")\n",
    ")\n",
    "\n",
    "songplays_temp = log_raw.df.join(\n",
    "        songs_temp,\n",
    "        [\n",
    "            log_raw.df.song == songs_temp.title,\n",
    "            log_raw.df.artist == songs_temp.name,\n",
    "            log_raw.df.length == songs_temp.duration\n",
    "        ],\n",
    "        \"left\"\n",
    ")\n",
    "\n",
    "songplays = DerivativeDF(songplays_temp\n",
    "        .join(time.df, \"start_time\", \"left\")\n",
    "        .select(\n",
    "            \"start_time\",\n",
    "            col(\"userId\").alias(\"user_id\"),\n",
    "            \"level\",\n",
    "            \"song_id\",\n",
    "            \"artist_id\",\n",
    "            col(\"sessionId\").alias(\"session_id\"),\n",
    "            \"location\",\n",
    "            col(\"userAgent\").alias(\"user_agent\"),\n",
    "            \"year\",\n",
    "            \"month\"\n",
    "        )\n",
    "        .withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "songplays._write_to_parquet(\n",
    "    s3_output_path=f\"s3://{output_bucket_name}/songplays\",\n",
    "    partitions=[\"year\", \"month\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Full ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, s3_raw_data_path:str, output_bucket_name:str):\n",
    "    \n",
    "    \n",
    "    def get_song_schema():\n",
    "        return StructType([\n",
    "                StructField(\"artist_id\", StringType(), False),\n",
    "                StructField(\"artist_latitude\", StringType(), True),\n",
    "                StructField(\"artist_longitude\", StringType(), True),\n",
    "                StructField(\"artist_location\", StringType(), True),\n",
    "                StructField(\"artist_name\", StringType(), False),\n",
    "                StructField(\"song_id\", StringType(), False),\n",
    "                StructField(\"title\", StringType(), False),\n",
    "                StructField(\"duration\", DoubleType(), False),\n",
    "                StructField(\"year\", IntegerType(), False)\n",
    "        ])\n",
    "\n",
    "    def create_song_table(song_raw, output_bucket_name:str):\n",
    "        songs = DerivativeDF(song_raw.df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"))\n",
    "        songs._write_to_parquet(\n",
    "            s3_output_path=f\"s3://{output_bucket_name}/songs\",\n",
    "            partitions=[\"year\", \"artist_id\"]\n",
    "            )\n",
    "        \n",
    "        return songs\n",
    "    \n",
    "    \n",
    "    def create_artists_table(song_raw, output_bucket_name:str):\n",
    "        artists = DerivativeDF(song_raw.df\n",
    "            .select(\n",
    "                \"artist_id\",\n",
    "                col(\"artist_name\").alias(\"name\"),\n",
    "                col(\"artist_location\").alias(\"location\"),\n",
    "                col(\"artist_latitude\").alias(\"latitude\"),\n",
    "                col(\"artist_longitude\").alias(\"longitude\"))\n",
    "            .distinct()\n",
    "        )\n",
    "        artists._write_to_parquet(\n",
    "            s3_output_path=f\"s3://{output_bucket_name}/artists\",\n",
    "            partitions=[\"artist_id\"]\n",
    "            )\n",
    "        \n",
    "        return artists\n",
    "    \n",
    "    logger.info(f\"Reading and Processing `{s3_raw_data_path}`\")\n",
    "    song_raw = RawDF(spark, s3_raw_data_path, get_song_schema())\n",
    "    logger.info(\"Processing and writting `songs` data\")\n",
    "    songs = create_song_table(song_raw, output_bucket_name)\n",
    "    logger.info(\"Processing and writting `artists` data\")\n",
    "    artists = create_artists_table(song_raw, output_bucket_name)\n",
    "    \n",
    "    return songs, artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, s3_raw_data_path:str, output_bucket_name:str, songs:DerivativeDF, artists:DerivativeDF):\n",
    "    \n",
    "    def get_log_schema():\n",
    "        return StructType([\n",
    "            StructField(\"artist\", StringType(), True),\n",
    "            StructField(\"auth\", StringType(), False),\n",
    "            StructField(\"firstName\", StringType(), True),\n",
    "            StructField(\"gender\", StringType(), True),\n",
    "            StructField(\"itemInSession\", IntegerType(), False),\n",
    "            StructField(\"lastName\", StringType(), True),\n",
    "            StructField(\"length\", DoubleType(), True),\n",
    "            StructField(\"level\", StringType(), False),\n",
    "            StructField(\"location\", StringType(), True),\n",
    "            StructField(\"method\", StringType(), False),\n",
    "            StructField(\"page\", StringType(), False),\n",
    "            StructField(\"registration\", DoubleType(), True),\n",
    "            StructField(\"sessionId\", IntegerType(), False),\n",
    "            StructField(\"song\", StringType(), True),\n",
    "            StructField(\"status\", IntegerType(), False),\n",
    "            StructField(\"ts\", DoubleType(), False),\n",
    "            StructField(\"userAgent\", StringType(), True),\n",
    "            StructField(\"userId\", StringType(), True)\n",
    "        ])\n",
    "    \n",
    "    get_datetime = udf(\n",
    "        lambda x: datetime.fromtimestamp(x / 1000).replace(microsecond=0),\n",
    "        TimestampType()\n",
    "    )\n",
    "    \n",
    "    def create_users_table(log_raw, output_bucket_name:str):\n",
    "        users = DerivativeDF(log_raw.df\n",
    "                        .withColumn(\"max_ts_user\", max(\"ts\").over(Window.partitionBy(\"userID\")))\n",
    "                        .filter(\n",
    "                            (col(\"ts\") == col(\"max_ts_user\")) &\n",
    "                            (col(\"userID\") != \"\") &\n",
    "                            (col(\"userID\").isNotNull())\n",
    "                        )\n",
    "                        .select(\n",
    "                            col(\"userID\").alias(\"user_id\"),\n",
    "                            col(\"firstName\").alias(\"first_name\"),\n",
    "                            col(\"lastName\").alias(\"last_name\"),\n",
    "                            \"gender\",\n",
    "                            \"level\"\n",
    "                        )\n",
    "            )\n",
    "        users._write_to_parquet(\n",
    "            s3_output_path=f\"s3://{output_bucket_name}/users\",\n",
    "            partitions=[\"level\"]\n",
    "            )\n",
    "        \n",
    "        return users\n",
    "    \n",
    "    def create_time_table(log_raw, output_bucket_name:str):\n",
    "        time = DerivativeDF(log_raw.df\n",
    "                            .withColumn(\"hour\", hour(\"start_time\"))\n",
    "                            .withColumn(\"day\", dayofmonth(\"start_time\"))\n",
    "                            .withColumn(\"week\", weekofyear(\"start_time\"))\n",
    "                            .withColumn(\"month\", month(\"start_time\"))\n",
    "                            .withColumn(\"year\", year(\"start_time\"))\n",
    "                            .withColumn(\"weekday\", dayofweek(\"start_time\"))\n",
    "                            .select(\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")\n",
    "                            .distinct()\n",
    "                             )\n",
    "        \n",
    "        time._write_to_parquet(\n",
    "            s3_output_path=f\"s3://{output_bucket_name}/time\",\n",
    "            partitions=[\"year\", \"month\"]\n",
    "            )\n",
    "        \n",
    "        return time\n",
    "        \n",
    "    \n",
    "    def create_songsplay_table(log_raw, songs, artists, output_bucket_name:str):\n",
    "        songs_temp = (\n",
    "                songs.df\n",
    "                .join(artists.df, \"artist_id\", \"full\")\n",
    "                .select(\"song_id\", \"title\", \"artist_id\", \"name\", \"duration\")\n",
    "        )\n",
    "\n",
    "        songplays_temp = log_raw.df.join(\n",
    "                songs_temp,\n",
    "                [\n",
    "                    log_raw.df.song == songs_temp.title,\n",
    "                    log_raw.df.artist == songs_temp.name,\n",
    "                    log_raw.df.length == songs_temp.duration\n",
    "                ],\n",
    "                \"left\"\n",
    "        )\n",
    "\n",
    "        songplays = DerivativeDF(songplays_temp\n",
    "                .join(time.df, \"start_time\", \"left\")\n",
    "                .select(\n",
    "                    \"start_time\",\n",
    "                    col(\"userId\").alias(\"user_id\"),\n",
    "                    \"level\",\n",
    "                    \"song_id\",\n",
    "                    \"artist_id\",\n",
    "                    col(\"sessionId\").alias(\"session_id\"),\n",
    "                    \"location\",\n",
    "                    col(\"userAgent\").alias(\"user_agent\"),\n",
    "                    \"year\",\n",
    "                    \"month\"\n",
    "                )\n",
    "                .withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "        )\n",
    "        \n",
    "        songplays._write_to_parquet(\n",
    "            s3_output_path=f\"s3://{output_bucket_name}/songplays\",\n",
    "            partitions=[\"year\", \"month\"]\n",
    "            )\n",
    "        \n",
    "        return songplays\n",
    "\n",
    "\n",
    "    logger.info(f\"Reading and Processing `{s3_raw_data_path}`\")\n",
    "    log_raw = RawDF(spark, s3_raw_data_path, get_log_schema())\n",
    "    log_raw.df = log_raw.df.filter(col(\"page\") == \"NextSong\")\n",
    "    log_raw.df = log_raw.df.withColumn(\"start_time\", get_datetime(\"ts\"))\n",
    "    \n",
    "    logger.info(\"Processing and writting `users` data\")\n",
    "    users = create_users_table(log_raw, output_bucket_name)\n",
    "    logger.info(\"Processing and writting `time` data\")\n",
    "    time = create_time_table(log_raw, output_bucket_name)\n",
    "    logger.info(\"Processing and writting `songplays` data\")\n",
    "    songplays = create_songsplay_table(log_raw, songs, artists, output_bucket_name)\n",
    "    \n",
    "    return users, time, songplays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sparkify_etl(APP:str='sparkify', STAGE:str='dev', bucket_exists:bool=True):\n",
    "    # If running on EMR cluster, name spark is available from the get-go\n",
    "#     try:\n",
    "#         spark\n",
    "#     except:\n",
    "#         spark = create_spark_session()\n",
    "    \n",
    "    raw_data_bucket_name = \"udacity-dend\"\n",
    "    song_data_path =  f\"s3a://{raw_data_bucket_name}/song_data/*/*/*\"\n",
    "    log_data_path = f\"s3a://{raw_data_bucket_name}/log_data/*/*\"\n",
    "    \n",
    "    song_data_path =  f\"s3a://{raw_data_bucket_name}/song_data/*/*/*\"\n",
    "    log_data_path = f\"s3a://{raw_data_bucket_name}/log_data/*/*\"\n",
    "    \n",
    "    output_bucket_name =  f'{APP}-{STAGE}'\n",
    "    if not bucket_exists:\n",
    "        logger.info(f\"Creating Bucket: `{output_bucket_name}`\")\n",
    "        create_bucket(output_bucket_name)\n",
    "    \n",
    "    logger.info(f\"Running Sparkigy ETL.\\n \\\n",
    "                  Reading raw data from `{raw_data_bucket_name}` \\n\\\n",
    "                  Writting output to `{output_bucket_name}`\"\n",
    "               )\n",
    "    logger.info(\"Processing Song Data\")\n",
    "    songs, artists = process_song_data(spark, song_data_path, output_bucket_name)\n",
    "    logger.info(\"Processing Log Data\")\n",
    "    users, time, songplays = process_log_data(spark, log_data_path, output_bucket_name, songs, artists)\n",
    "    logger.info(\"Sparkify ETL is completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sparkify_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
